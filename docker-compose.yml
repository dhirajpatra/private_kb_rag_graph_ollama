version: '3.8'

services:
  rag_service:
    build: ./rag_service
    environment:
      - PYTHONPATH=/app  # ✅ Ensure Python finds the module
    command: uvicorn app:app --host 0.0.0.0 --port 5000
    volumes:
      - ./rag_service:/app
    working_dir: /app
    ports:
      - "5000:5000"
    depends_on:
      - ollama_server_lite
      # - ollama_server
    deploy:
      resources:
        limits:
          cpus: '2.00'
          memory: 4G
    networks:
      - rag-network

  ollama_server_lite:
    image: langgraph_example-ollama_server_lite:latest  # or build with llama3.2:1b
    build: 
      context: ./ollama_server
    container_name: ollama_server_lite
    ports:
      - "11435:11434"
    volumes:
      - .:/app
      - ./ollama_data:/root/.ollama  # ✅ Changed from direct /root path to local folder
      - ./ollama_models:/root/.ollama/models
    entrypoint: ["/bin/bash", "/app/ollama_server/entrypoint.sh"]
    deploy:
      resources:
        limits:
          cpus: '2.00'
          memory: 8G
    networks:
      - rag-network

  # ollama_server:
  #   # image: ai_agent_project/ollama_server:latest # deepseek-r1:1.5b
  #   image: langgraph_example-ollama_server:latest # llama3.1:8b or qwen2.5:7b-instruct
  #   # build:
  #   #   context: ./ollama_server
  #   container_name: ollama_server
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - .:/app
  #     - ./ollama_data:/root/.ollama  # ✅ Changed from direct /root path to local folder
  #     - ./ollama_models:/root/.ollama/models
  #   entrypoint: ["/bin/bash", "/app/ollama_server/entrypoint.sh"]
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '2.00'
  #         memory: 8G
  #   networks:
  #     - agent-network

  streamlit_app:
    build: ./chat
    container_name: streamlit_app
    volumes:
      - .:/app
    ports:
      - "8501:8501"
    depends_on:
      - ollama_server_lite
      # - ollama_server
      - rag_service
    deploy:
      resources:
        limits:
          cpus: '1.00'
          memory: 2G
    networks:
      - rag-network

volumes:
  ollama_data:
  ollama_models:
  
networks:
  rag-network:
    driver: bridge